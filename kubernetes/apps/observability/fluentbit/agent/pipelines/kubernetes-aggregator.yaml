pipeline:
  inputs:
    - name: kafka
      brokers: kafka-cluster-kafka-brokers.databases.svc.cluster.local:9093
      topics: cluster-apps-logs
      rdkafka.client.id: fluent-bit-aggregator
      rdkafka.enable.ssl.certificate.verification: true
      rdkafka.ssl.certificate.location: /fluent-bit/etc/secrets/user.crt
      rdkafka.ssl.key.location: /fluent-bit/etc/secrets/user.key
      rdkafka.ssl.ca.location: /fluent-bit/etc/secrets/ca.crt
      rdkafka.security.protocol: ssl
      format: none
      #rdkafka.fetch.message.max.bytes: 5242880       # 5 MB, must match agent message.max.bytes
      #rdkafka.receive.message.max.bytes: 5242880
      #rdkafka.max.partition.fetch.bytes: 5242880     # 5 MB, also ensures partitions can fetch large messages


  filters:
    - name: kubernetes
      match: kubernetes.*
      merge_log: true
      kube_tag_prefix: kubernetes.var.log.containers.
      k8s-logging.parser: true
      k8s-logging.exclude: true
      namespace_labels: false
      annotations: false
      buffer_size: 64kb
    - name: nest
      match: '*'
      operation: lift
      nested_under: kubernetes
      add_prefix: k_
    - name: nest
      match: '*'
      operation: lift
      nested_under: k_labels
      add_prefix: k_labels_
    - name: modify
      match: '*'
      rename: k_labels_app.kubernetes.io/name app
      rename: k_labels_app app
      rename: k_labels_k8s-app app

  outputs:
    - name: http
      match: '*'
      host: victoria-logs-server.observability.svc.cluster.local
      port: 9428
      uri: /insert/jsonline?_stream_fields=stream,k_namespace_name,k_pod_name,app&_msg_field=log&_time_field=date
      format: json_lines
      json_date_format: iso8601
      compress: gzip
      log_response_payload: false
