pipeline:
  inputs:
    - name: kafka
      brokers: kafka-cluster-kafka-brokers.databases.svc.cluster.local:9093
      topics: cluster-apps-logs
      rdkafka.client.id: fluent-bit-aggregator
      rdkafka.enable.ssl.certificate.verification: true
      rdkafka.ssl.certificate.location: /fluent-bit/etc/secrets/user.crt
      rdkafka.ssl.key.location: /fluent-bit/etc/secrets/user.key
      rdkafka.ssl.ca.location: /fluent-bit/etc/secrets/ca.crt
      rdkafka.security.protocol: ssl
      format: json

  filters:
    - name: lua
      match: '*'
      #script: kafka.lua
      call: modify_kafka_message
      code: |
        local count = 0

        -- Recursive function to get the deepest payload
        function flatten_payload(p)
            while p.payload do
                p = p.payload
            end
            return p
        end

        function modify_kafka_message(tag, timestamp, record)
            count = count + 1

            -- Flatten the payload recursively
            local flat = flatten_payload(record)

            -- Add status and topic at top level
            flat.status = 'processed by fluent-bit, total records: '..tostring(count)
            flat.topic = record.topic or flat.topic

            -- Return as the new record
            return 1, timestamp, flat
        end

  outputs:
    - name: http
      match: '*'
      host: victoria-logs-server.observability.svc.cluster.local
      port: 9428
      uri: /insert/jsonline?_stream_fields=stream,k_namespace_name,k_pod_name,app&_msg_field=log&_time_field=date
      format: json_lines
      json_date_format: iso8601
      compress: gzip
      log_response_payload: false
